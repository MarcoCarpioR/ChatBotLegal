import streamlit as st
import os
import json
import faiss
import pickle
import numpy as np

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. CARGAR CREDENCIALES GCP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open("gcp_key.json", "w") as f:
    json.dump(json.loads(st.secrets["GOOGLE_CREDENTIALS_JSON"]), f)

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "gcp_key.json"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. IMPORTAR MODELOS DE VERTEX AI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel
from vertexai.preview.language_models import TextEmbeddingModel

aiplatform.init(
    project=st.secrets["GCP_PROJECT"],
    location=st.secrets["GCP_REGION"]
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. CARGAR FAISS + METADATOS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def cargar_index_y_datos():
    index = faiss.read_index("index_normas.faiss")
    with open("metadata.pkl", "rb") as f:
        data = pickle.load(f)
    return index, data["textos"], data["fuentes"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. CARGAR MODELOS GEMINI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def cargar_modelos():
    embed_model = TextEmbeddingModel.from_pretrained("textembedding-gecko@001")
    chat_model = GenerativeModel("gemini-2.5-pro")
    return embed_model, chat_model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. FUNCIONES DE CONSULTA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def embed_text(modelo, texto):
    vector = modelo.get_embeddings([texto])[0].values
    return np.array(vector, dtype="float32").reshape(1, -1)

def buscar_contexto(pregunta, modelo_embed, index, textos, fuentes, k=1):
    vector = embed_text(modelo_embed, pregunta)
    distancias, indices = index.search(vector, k)
    resultados = [(textos[i], fuentes[i]) for i in indices[0]]
    return resultados

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. UI STREAMLIT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ¦· ChatClÃ­nica Legal")
st.markdown("Consulta normativa para habilitaciÃ³n de consultorios odontolÃ³gicos en PerÃº.")

pregunta = st.text_input("ğŸ” Escribe tu pregunta legal:")

if pregunta:
    with st.spinner("Buscando normativa relevante..."):
        index, textos, fuentes = cargar_index_y_datos()
        modelo_embed, modelo_chat = cargar_modelos()

        fragmentos = buscar_contexto(pregunta, modelo_embed, index, textos, fuentes, k=1)
        contexto, fuente = fragmentos[0]

        prompt = f"""
Eres un asistente legal para consultorios odontolÃ³gicos en PerÃº.
Responde la siguiente consulta en base a la normativa entregada.

--- CONTEXTO ---
{contexto}
--- FIN DEL CONTEXTO ---

Pregunta: {pregunta}
Respuesta:
"""

        respuesta = modelo_chat.generate_content(prompt).text.strip()

    st.success("âœ… Respuesta:")
    st.write(respuesta)
    st.info(f"ğŸ“ Fuente: {fuente}")

    with st.expander("ğŸ“„ Fragmento normativo usado"):
        st.code(contexto)
